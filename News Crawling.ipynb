{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Web Crawling Practice - News Website\n",
    " -Grace Liu, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "import re  \n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the website - The Japan Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "date, title, urls= [], [], []\n",
    "\n",
    "for i in range(1,24): #24 \n",
    "    url = \"https://www.japantimes.co.jp/news/tech/page/\" + str(i)+\"/\"\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}\n",
    "    response = requests.get(url, headers =headers)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    \n",
    "    #date & title\n",
    "    date.extend([tag.text for tag in soup.find_all(\"span\", class_ = \"right date\")])\n",
    "    for i in range(12):\n",
    "        title.append(soup.find_all(\"header\")[i+1].find(\"p\").find(\"a\").text)\n",
    "        urls.append(soup.find_all(\"header\")[i+1].find(\"p\").find(\"a\")[\"href\"])\n",
    "\n",
    "# dont want the first three coz they dont have the \"date\"\n",
    "title = title[3:]\n",
    "urls = urls[3:]\n",
    "\n",
    "df = pd.DataFrame(data={ \"Date\": date, \"Title\": title, \"Url\": urls }) \n",
    "\n",
    "#Save it\n",
    "#df.to_csv(\"japan_news_2018.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dec 24, 2018</td>\n",
       "      <td>What happens when Alexa gets too smart or too ...</td>\n",
       "      <td>https://www.japantimes.co.jp/news/2018/12/24/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dec 24, 2018</td>\n",
       "      <td>Huawei gear pulled from $3 billion U.K. police...</td>\n",
       "      <td>https://www.japantimes.co.jp/news/2018/12/24/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dec 18, 2018</td>\n",
       "      <td>Lovot, Japan's new touchy-feely family robot, ...</td>\n",
       "      <td>https://www.japantimes.co.jp/news/2018/12/18/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dec 17, 2018</td>\n",
       "      <td>Seven-Eleven tries out a store in Tokyo with u...</td>\n",
       "      <td>https://www.japantimes.co.jp/news/2018/12/17/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dec 17, 2018</td>\n",
       "      <td>Japanese electronics firms look to re-engineer...</td>\n",
       "      <td>https://www.japantimes.co.jp/news/2018/12/17/b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date                                              Title  \\\n",
       "0  Dec 24, 2018  What happens when Alexa gets too smart or too ...   \n",
       "1  Dec 24, 2018  Huawei gear pulled from $3 billion U.K. police...   \n",
       "2  Dec 18, 2018  Lovot, Japan's new touchy-feely family robot, ...   \n",
       "3  Dec 17, 2018  Seven-Eleven tries out a store in Tokyo with u...   \n",
       "4  Dec 17, 2018  Japanese electronics firms look to re-engineer...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://www.japantimes.co.jp/news/2018/12/24/b...  \n",
       "1  https://www.japantimes.co.jp/news/2018/12/24/b...  \n",
       "2  https://www.japantimes.co.jp/news/2018/12/18/b...  \n",
       "3  https://www.japantimes.co.jp/news/2018/12/17/b...  \n",
       "4  https://www.japantimes.co.jp/news/2018/12/17/b...  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the website - The Korea Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "date, title, content, urls= [], [], [],[]\n",
    "\n",
    "import time\n",
    "start = time.time()   #記錄時間\n",
    "for i in range(1,92):\n",
    "    url = \"http://www.koreatimes.co.kr/www/sublist_133_\" + str(i)+\".html\"\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}\n",
    "    response = requests.get(url, headers =headers)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    \n",
    "    #date & title\n",
    "    date.extend([tag.text.split(\"\\n\")[1] for tag in soup.find_all(\"div\", class_ = \"list_article_byline2\")])  ##Not yet\n",
    "    title.extend([tag.text for tag in soup.find_all(\"div\", class_ = \"list_article_headline HD\")])\n",
    "    content.extend([tag.text for tag in soup.find_all(\"div\", class_ = \"list_article_lead LD\")])\n",
    "    urls.extend([\"http://www.koreatimes.co.kr\" + tag.find(\"a\")[\"href\"] for tag in soup.find_all(\"div\", class_ = \"list_article_lead LD\")])\n",
    "\n",
    "df = pd.DataFrame(data={ \"Date\": date, \"Title\": title, \"Content\":content, \"Url\": urls }) \n",
    "\n",
    "end = time.time()   #記錄時間\n",
    "#Save it\n",
    "df.to_csv(\"skorea_news_2018.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-28 17:08</td>\n",
       "      <td>High-speed internet to be available in rural a...</td>\n",
       "      <td>High-speed internet services will be extended ...</td>\n",
       "      <td>http://www.koreatimes.co.kr/www/tech/2018/12/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-27 17:59</td>\n",
       "      <td>Naver, Kakao face growing union risks</td>\n",
       "      <td>Naver and Kakao, both expecting to grapple wit...</td>\n",
       "      <td>http://www.koreatimes.co.kr/www/tech/2018/12/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-26 17:42</td>\n",
       "      <td>Samsung employees volunteer to help neighbors ...</td>\n",
       "      <td>Samsung Electronics employees have implemented...</td>\n",
       "      <td>http://www.koreatimes.co.kr/www/tech/2018/12/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-26 17:41</td>\n",
       "      <td>Huawei seeks to become responsible community m...</td>\n",
       "      <td>Huawei Technologies is a global leading networ...</td>\n",
       "      <td>http://www.koreatimes.co.kr/www/tech/2018/12/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-25 17:05</td>\n",
       "      <td>LG Uplus CEO to attend CES to scout new biz ch...</td>\n",
       "      <td>LG Uplus CEO and Vice Chairman Ha Hyun-hwoi wi...</td>\n",
       "      <td>http://www.koreatimes.co.kr/www/tech/2018/12/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date                                              Title  \\\n",
       "0  2018-12-28 17:08  High-speed internet to be available in rural a...   \n",
       "1  2018-12-27 17:59             Naver, Kakao face growing union risks    \n",
       "2  2018-12-26 17:42  Samsung employees volunteer to help neighbors ...   \n",
       "3  2018-12-26 17:41  Huawei seeks to become responsible community m...   \n",
       "4  2018-12-25 17:05  LG Uplus CEO to attend CES to scout new biz ch...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  High-speed internet services will be extended ...   \n",
       "1  Naver and Kakao, both expecting to grapple wit...   \n",
       "2  Samsung Electronics employees have implemented...   \n",
       "3  Huawei Technologies is a global leading networ...   \n",
       "4  LG Uplus CEO and Vice Chairman Ha Hyun-hwoi wi...   \n",
       "\n",
       "                                                 Url  \n",
       "0  http://www.koreatimes.co.kr/www/tech/2018/12/1...  \n",
       "1  http://www.koreatimes.co.kr/www/tech/2018/12/1...  \n",
       "2  http://www.koreatimes.co.kr/www/tech/2018/12/1...  \n",
       "3  http://www.koreatimes.co.kr/www/tech/2018/12/1...  \n",
       "4  http://www.koreatimes.co.kr/www/tech/2018/12/1...  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
